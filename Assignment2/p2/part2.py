# -*- coding: utf-8 -*-
"""DL_A2_P2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k8wnJqzieOWEcsDM1rGavooIXCZgwWPI

Dataset: Mini Daily Dialog - Use train.csv and test.csv file for the assignment.
Note: Data trimming is not allowed in this question.

Dataset description: The dataset is a sample of widely used DailyDialog dataset for dialogue-act classification task.

Task: Dialogue-act Classification: 

For a given dialogue students need to develop a program to predict act of utterance at time T with the help of previous X utterances as context.

1. Visualize dialogue corpus and show stats of the train and test file. [2 Marks]

2. Implement a program using just LSTM and linear layers to predict act of utterance at time T considering previous X utterancesâ€™ context. [8 Marks]
Note: Students need to propose an architecture for this.

3. Now, show plots for accuracy and weighted F1 scores for X = {0,1,2,3,4} [10 Marks]

4. Does the performance of the model increase with increase in X? Justify. [5 Marks]

Expected deliverables of the assignment 

- For every question within each part, visualize the learning using the following plots:
- Training Loss vs Number of Epochs
- Validation Loss vs Number of Epochs
- Plots showing convergence over different values of X
- Save the models in pickles. Students will be asked to reproduce results using saved models only.
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')
import nltk
nltk.download('punkt')
nltk.download('stopwords')
 
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from string import punctuation

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

folder_path = "/content/drive/MyDrive/DL_A2"
train_file = folder_path + '/train.csv' 
test_file = folder_path + '/test.csv'

train_data = pd.read_csv(train_file, header=0, names=["utterance", "act"])
test_data = pd.read_csv(test_file, header=0, names=["utterance", "act"])

"""# Part 1: EDA

## 1.1. Visualisation
"""

train_data.shape, test_data.shape

train_data.head(10)

test_data.sample(10)

train_data.groupby("act").count()

test_data.groupby("act").count()

train_data.shape, test_data.shape

train_data["act"].hist()

test_data.act.hist()

"""## 1.2. Data Cleaning"""

def remove_stop(strings, stop_list):
    classed= [s for s in strings if s not in stop_list]
    return classed

def normalize(text):
    return " ".join(text)

ytrain = np.array(train_data.act).reshape(-1,1)
ytest  = np.array(test_data.act).reshape(-1,1)

y_encoder = OneHotEncoder().fit(np.array(train_data.act).reshape(-1,1))
ytrain_encoded = y_encoder.transform(np.array(train_data.act).reshape(-1,1)).toarray()
ytest_encoded  = y_encoder.transform(np.array(test_data.act).reshape(-1,1)).toarray()

train_data["all_lower"]= train_data.utterance.map(lambda x: x.lower())
test_data["all_lower"]= test_data.utterance.map(lambda x: x.lower())

train_data["tokenized"]= train_data.all_lower.map(word_tokenize)
test_data["tokenized"]= test_data.all_lower.map(word_tokenize)

stop = stopwords.words("english")
stop_punc = list(set(punctuation))+ stop

train_data["refine"]= train_data.tokenized.map(lambda df: remove_stop(df, stop_punc))
test_data["refine"]= test_data.tokenized.map(lambda df: remove_stop(df, stop_punc))

stemmer= PorterStemmer()

train_data["stemmed"]= train_data.refine.map(lambda xs: [stemmer.stem(x) for x in xs])
train_data["normalized"]= train_data.stemmed.apply(normalize)

test_data["stemmed"]= test_data.refine.map(lambda xs: [stemmer.stem(x) for x in xs])
test_data["normalized"]= test_data.stemmed.apply(normalize)

train_data.normalized

tokenizer= Tokenizer()
tokenizer.fit_on_texts(train_data.normalized)

tokenized_train = tokenizer.texts_to_sequences(train_data.normalized)
tokenized_test = tokenizer.texts_to_sequences(test_data.normalized)

tokenizer.word_index.keys().__len__()

train_padded = pad_sequences(tokenized_train, maxlen=10, padding = "pre")
test_padded = pad_sequences(tokenized_test, maxlen=10, padding = "pre")

train_padded.shape, test_padded.shape

def transform_x(data, tokenizer, x):
    output_shape= [data.shape[0], x, tokenizer.word_index.keys().__len__()]
    results= np.zeros(output_shape)
    
    for i in range(data.shape[0]):
        for ii in range(x):
            results[i, ii, data[i,ii]-1]= 1
            
    return results

"""# Part 2: NN - LSTM + Linear Layers

## 2.1: Implement LSTM
"""

class LSTMLinear(object):
    def __init__(self):
        self.loss= tf.keras.losses.CategoricalCrossentropy()
        self.metrics= tf.keras.metrics.AUC()
        self.optimizer= tf.keras.optimizers.Adam()        

    def get_model(self, input_dim, output_shape, steps):
        self.input_dim = input_dim
        self.output_shape = output_shape
        self.steps = steps
        
        model = self.build_model()
        model = self.compile_model(model)
        
        return model
        
    def build_model(self):
        
        input_layer= tf.keras.layers.Input(shape=(self.steps, self.input_dim))
        x = tf.keras.layers.LSTM(units=self.steps+1)(input_layer)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dense(self.output_shape*2)(x)
        x = tf.keras.layers.Dense(self.output_shape)(x)
        output= tf.keras.activations.softmax(x, axis= 1)
 
        model= tf.keras.Model(inputs= [input_layer], outputs= [output])
        
        return model
    
    def compile_model(self, model):

        model.compile(optimizer= self.optimizer, loss= self.loss, metrics= [self.metrics])
        
        return model

model_dir = LSTMLinear()

"""# Part 3: Train LSTM for different steps

## Part 3.1: Train for X=0
"""

Xtrain0 = train_padded.copy()
Xtest0 = test_padded.copy()

Xtrain0 = Xtrain0[:, np.newaxis, :]
Xtest0 = Xtest0[:, np.newaxis, :]

Xtrain0.shape , Xtest0.shape

steps = Xtrain0.shape[1]
dim = Xtrain0.shape[2]
output_shape = ytrain_encoded.shape[1]
epochs = 10

dim

model = model_dir.get_model(input_dim= dim,
                      output_shape= output_shape,
                      steps= 1)
model.summary()

history = model.fit(Xtrain0, ytrain_encoded, validation_split= 0.2, epochs= epochs)

"""### Training Curves"""

auc = history.history['auc']
val_auc = history.history['val_auc']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, auc, label='Training AUC')
plt.plot(epochs_range, val_auc, label='Validation AUC')
plt.legend(loc='upper right')
plt.title('Training and Validation AUC for LSTM at X=0')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss for LSTM at X=0')
plt.show()

"""### Training evaluation"""

prediction_train = y_encoder.inverse_transform(model.predict(Xtrain0))
print(classification_report(train_data.act, prediction_train))

"""### Testing Evaluation"""

prediction_test= y_encoder.inverse_transform(model.predict(Xtest0))
print(classification_report(test_data.act, prediction_test))

model.save("X0")
!zip -r /content/X0.zip /content/X0
from google.colab import files
files.download('/content/X0.zip')

"""## Part 3.2: Train for X=1

"""

Xtrain1 = transform_x(train_padded, tokenizer, x=1)
Xtest1 = transform_x(test_padded, tokenizer, x=1)

Xtrain1.shape , Xtest1.shape

steps = Xtrain1.shape[1]
dim = Xtrain1.shape[2]
output_shape = ytrain_encoded.shape[1]
epochs = 10

model = model_dir.get_model(input_dim= dim,
                             output_shape= output_shape,
                             steps=steps)
model.summary()

history = model.fit(Xtrain1, ytrain_encoded, validation_split= 0.2, epochs= epochs)

"""### Training Curves"""

auc = history.history['auc']
val_auc = history.history['val_auc']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, auc, label='Training AUC')
plt.plot(epochs_range, val_auc, label='Validation AUC')
plt.legend(loc='upper right')
plt.title('Training and Validation AUC for LSTM at X=1')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss for LSTM at X=1')
plt.show()

"""### Training evaluation"""

prediction_train = y_encoder.inverse_transform(model.predict(Xtrain1))
print(classification_report(train_data.act, prediction_train))

"""### Testing Evaluation"""

prediction_test = y_encoder.inverse_transform(model.predict(Xtest1))
print(classification_report(test_data.act, prediction_test))

model.save("X1")
!zip -r /content/X1.zip /content/X1
from google.colab import files
files.download('/content/X1.zip')

"""## Part 3.3: Train for X=2

"""

Xtrain2 = transform_x(train_padded, tokenizer, x=2)
Xtest2 = transform_x(test_padded, tokenizer, x=2)

Xtrain2.shape , Xtest2.shape

steps = Xtrain2.shape[1]
dim = Xtrain2.shape[2]
output_shape = ytrain_encoded.shape[1]
epochs = 10

model = model_dir.get_model(input_dim= dim,
                      output_shape= output_shape,
                      steps= steps)
model.summary()

history = model.fit(Xtrain2, ytrain_encoded, validation_split= 0.2, epochs= epochs)

"""### Training Curves"""

auc = history.history['auc']
val_auc = history.history['val_auc']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, auc, label='Training AUC')
plt.plot(epochs_range, val_auc, label='Validation AUC')
plt.legend(loc='upper right')
plt.title('Training and Validation AUC for LSTM at X=2')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss for LSTM at X=2')
plt.show()

"""### Training evaluation"""

prediction_train = y_encoder.inverse_transform(model.predict(Xtrain2))
print(classification_report(train_data.act, prediction_train))

"""### Testing Evaluation"""

prediction_test = y_encoder.inverse_transform(model.predict(Xtest2))
print(classification_report(test_data.act, prediction_test))

model.save("X2")
!zip -r /content/X2.zip /content/X2
from google.colab import files
files.download('/content/X2.zip')

"""## Part 3.4: Train for X=3

"""

Xtrain3 = transform_x(train_padded, tokenizer, x=3)
Xtest3 = transform_x(test_padded, tokenizer, x=3)

Xtrain3.shape , Xtest3.shape

steps = Xtrain3.shape[1]
dim = Xtrain3.shape[2]
output_shape = ytrain_encoded.shape[1]
epochs = 10

model = model_dir.get_model(input_dim= dim,
                      output_shape= output_shape,
                      steps= steps)
model.summary()

history = model.fit(Xtrain3, ytrain_encoded, validation_split= 0.2, epochs= epochs)

"""### Training Curves"""

auc = history.history['auc']
val_auc = history.history['val_auc']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, auc, label='Training AUC')
plt.plot(epochs_range, val_auc, label='Validation AUC')
plt.legend(loc='upper right')
plt.title('Training and Validation AUC for LSTM at X=3')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss for LSTM at X=3')
plt.show()

"""### Training evaluation"""

prediction_train = y_encoder.inverse_transform(model.predict(Xtrain3))
print(classification_report(train_data.act, prediction_train))

"""### Testing Evaluation"""

prediction_test = y_encoder.inverse_transform(model.predict(Xtest3))
print(classification_report(test_data.act, prediction_test))

model.save("X3")
!zip -r /content/X3.zip /content/X3
from google.colab import files
files.download('/content/X3.zip')

"""## Part 3.5: Train for X=4

"""

Xtrain4 = transform_x(train_padded, tokenizer, x=4)
Xtest4 = transform_x(test_padded, tokenizer, x=4)

Xtrain4.shape , Xtest4.shape

steps = Xtrain4.shape[1]
dim = Xtrain4.shape[2]
output_shape = ytrain_encoded.shape[1]

epochs = 10

model = model_dir.get_model(input_dim= dim,
                      output_shape= output_shape,
                      steps= steps)
model.summary()

history = model.fit(Xtrain4, ytrain_encoded, validation_split= 0.2, epochs= epochs)

"""### Training Curves"""

auc = history.history['auc']
val_auc = history.history['val_auc']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, auc, label='Training AUC')
plt.plot(epochs_range, val_auc, label='Validation AUC')
plt.legend(loc='upper right')
plt.title('Training and Validation AUC for LSTM at X=4')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss for LSTM at X=4')
plt.show()

"""### Training evaluation"""

prediction_train = y_encoder.inverse_transform(model.predict(Xtrain4))
print(classification_report(train_data.act, prediction_train))

"""### Testing Evaluation"""

prediction_test = y_encoder.inverse_transform(model.predict(Xtest4))
print(classification_report(test_data.act, prediction_test))

model.save("X4")
!zip -r /content/X4.zip /content/X4
from google.colab import files
files.download('/content/X4.zip')

"""# Part 4: Comparison

- We see that the model performance does not increase linearly with X. 
- Beyond X=2, the model starts overfitting. 
- This may be because of use of only Linear Layers in LSTM. 
- To increase accuracy, we need to add Bidirectional LSTM to increase model context for learning.
"""